# Teste: como diferentes LLMs geram código Python?

A intenção deste repo é documentar um teste que eu fiz de geração de código usando diferentes LLMs (locais e cloud).

## Modelos testados:

`executado na ☁️`

1. OpenAI: gpt4o
2. OpenAI: gpt4o-mini
3. Anthropic: Sonnet3.5
4. [novo] OpenAI: gpt-o1 | 1-shot
5. [novo] OpenAI: gpt-o1 | 2-shot

`executado localmente com Ollama `

6. Meta: llama3.1:8b | 4.7gb
7. Google: gemma2 | 5.4gb | 3-shot
8. Alibaba: qwen2.5-coder | 4.7gb


## O que tem no Repo?
Você vai encontrar um .md com o prompt completo.
E cada script (.py) gerado por cada uma das LLMs.

## Exemplos de respostas:
![image](https://github.com/user-attachments/assets/389867be-0cfc-4c37-87b9-527325d7f83a)

![image](https://github.com/user-attachments/assets/472a751b-ea0b-4b43-a26f-c9146fbd87f1)

![image](https://github.com/user-attachments/assets/52ec63c8-9ffc-4b64-bda3-a06f8e0f8e44)
